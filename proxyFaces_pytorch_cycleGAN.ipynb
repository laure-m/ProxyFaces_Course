{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRh+9gAXjvBl6/ufoEN+Ax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laure-m/ProxyFaces_Course/blob/main/proxyFaces_pytorch_cycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROXY FACES UCLA AUD SPRING TECH SEMINAR \n",
        "#CycleGAN (pytorch) Notebook\n",
        "This notebook will walkthrough all the steps for training and testing using cycleGAN. Take a look at the [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for more information"
      ],
      "metadata": {
        "id": "G7OxzFW63EZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***INITIAL SETUP***\n",
        "\n",
        "Mount to Drive"
      ],
      "metadata": {
        "id": "eFaehwtz3Ojz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQGVEyav252s"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone Repository"
      ],
      "metadata": {
        "id": "RRwpfAi23WrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
      ],
      "metadata": {
        "id": "f1EW8Zat3ZcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup + Install Requirements"
      ],
      "metadata": {
        "id": "FWGSNDmq4AwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
      ],
      "metadata": {
        "id": "YWC7Cbkt3dT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "O2o6sYh33eaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change Folder Directory"
      ],
      "metadata": {
        "id": "DxieKuRb4DK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/pytorch-CycleGAN-and-pix2pix"
      ],
      "metadata": {
        "id": "P8F9Wl7B3s5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***DATASETS***"
      ],
      "metadata": {
        "id": "CPnX28Zn3fqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Either Download this example dataset >> "
      ],
      "metadata": {
        "id": "qFrGtVh64j3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \n",
        "%cd /content/drive/MyDrive/pytorch-CycleGAN-and-pix2pix"
      ],
      "metadata": {
        "id": "0BWGivj94n29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR upload your own (must have appropriate folder structure, naming, and size)\n",
        "\n",
        "Create a dataset folder under /dataset for your dataset.\n",
        "Create subfolders testA, testB, trainA, and trainB under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the testA folder, images you want to transform from b to a (dog2cat) in the testB folder, and do the same for the trainA and trainB folders."
      ],
      "metadata": {
        "id": "Gqx2el1b4P9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TRAINING***\n",
        "\n",
        "\n",
        "python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan\n",
        "Change the --dataroot and --name to your own dataset's path and model's name. Use --gpu_ids 0,1,.. to train on multiple GPUs and --batch_size to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\n",
        "\n",
        "Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n",
        "\n",
        "Use cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth if you want to transform images from class A to class B and cp ./checkpoints/horse2zebra/latest_net_G_B.pth ./checkpoints/horse2zebra/latest_net_G.pth if you want to transform images from class B to class A."
      ],
      "metadata": {
        "id": "mbrozs8c3mWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --dataroot ./datasets/selfie2anime --name selfie2anime --model cycle_gan --display_id -1"
      ],
      "metadata": {
        "id": "QiWMdyZP3uja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TESTING***\n",
        "\n",
        "\n",
        "python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout\n",
        "Change the --dataroot and --name to be consistent with your trained model's configuration.\n",
        "\n",
        "from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix: The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n",
        "\n",
        "For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model."
      ],
      "metadata": {
        "id": "COJ_MgLS3wQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --dataroot datasets/selfie2anime/testA --name selfie2anime  --model test --no_dropout"
      ],
      "metadata": {
        "id": "A3PxIyb-31Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***VISUALIZE***"
      ],
      "metadata": {
        "id": "BPLo5Nhy32vZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread('./results/selfie2anime/test_latest/images/female_10328_fake.png')\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "d4Sg3Aii34-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread('./results/selfie2anime/test_latest/images/female_10328_real.png')\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "ke-kqoJh36ic"
      }
    }
  ]
}